# Neural Network Charity Analysis
DU Data Bootcamp: Module 19 (Deep Learning, Neural Networks and Tensorflow)

## Overview of Project

### Purpose of Project
Alphabet Soup has funded various philanthropic endeavors over the years, and their business team is interested to see if machine learning and data science can provide better insight into slection of future charitable giving contributions to hopefully ensure that their efforts result in healthy organizations that end up really having a positive impact based on their original intent.

## Analysis and Challenges

### Analysis of Outcomes Based on Initial Deep Learning Model
The initial Deep Learning Model made a few efforts at preprocessing the raw data provided by Alphabet Soup, but resulted in only about a 55% model accuracy. Deep Learning Models are highly favored for their analylitic power to be able to provide very high levels of accuracy. However, in order to acheive that accuracy, the preprocessing of the raw data is incredibly important. One of the key elements to review in the data is how wide (e.g. how many features) are being built into the dataset and then the model. It's also incredibly important to understand the data before preprocessing to understand what is really being communicated.

The target of this particular model was to measure/predict the successful outcome of the test dataset of charities. In the original model, the features were: the Alphabet Soup application type, the industry sector of the applying organizations, the government classification of those organizations, the indicated use of charitable funds, the type of organizations, the level of income of the charity, the amount of funding requested, and whether or not special considerations were taken into consideration. The initial variables removed from consideration was the tax identification number of the organization, and the org name.

### Deep Learning Model Optimization Strategies
The first approach I took to further process the data was to convert the organizational incomes from strings to numerical values. The level of income generated by the organizations may not typically be considered important for charities, however it may be useful for to get an indication of the sustainability of the organization after an initial round of funding has been received and utilized. I chose to use a measure of central tendency less impacted by extremes within the bins (median) from the strings in the raw data.

Additionally, while it might be of important record to understand which organizations received special consideration, the very fact that there were special circumstances indicated to me that perhaps that variable may not be the most useful in a deep learning model, since it is by definition, an outlier. Eliminating consideration of the use case of the funding also had a positive effect on the model, suggesting that features about the charity itself were more impactful than what the charity was actually trying to accomplish.

I also adjusted the bins for the application classification feature to better utilize the density plot to understand rare classification types. This seemed to aide in model accuracy. I attempted to adjust the bins for application type, but found that this feature was actually pretty important to model performance as set from the initial preprocessing attempt.

One adjustment to the model I tried that I then reverted was attempting to expand the model's hidden layers. Adding a third layer to the model was a significant downgrade to overall model accuracy. Utilizing different activation functions or varying optimizers did not seem to imporve the model, but increasing the number of neurons in each of the two hidden layers did demonstrate a postive result.

I tried a few data preprocessing strategies with other features, but ultimately my models with numerous attempts topped out at around 71% accuracy.

## Results
Overall, the results of my initial exploration into deep learning models have taught me that I have an incredible amount to learn. I am so curious about how the various activation functions and optimizers and other things might improve the accuracy of the models. I look forward to the years ahead of continuing to grow as I embark on this journey within data science, and look forward to all of the thrilling discoveries in front of me.